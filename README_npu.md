# InternLM-NPU

<div align="center">

<img src="./assets/logo.svg" width="200"/>
  <div>Â </div>
  <div align="center">
    <b><font size="5">InternLM</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>Â </div>
  </div>

[![license](./assets/license.svg)](./LICENSE)
[![evaluation](./assets/compass_support.svg)](https://github.com/internLM/OpenCompass/)

<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->

[ðŸ“˜Commercial Application](#license) |
[ðŸ¤—HuggingFace](https://huggingface.co/internlm) |
[ðŸ†•Update News](#news) |
[ðŸ¤”Reporting Issues](https://github.com/InternLM/InternLM/issues/new) |
[ðŸ“œTechnical Report](https://arxiv.org/abs/2403.17297)<br>
[ðŸ’¬Chat Web](https://internlm-chat.intern-ai.org.cn/) |
[ðŸ”—API](https://internlm.intern-ai.org.cn/api/document) |
[ðŸ§©Modelers](https://modelers.cn/spaces/MindSpore-Lab/INTERNLM2-20B-PLAN)

[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)

</div>


## Introduction

This is a guide to using Ascend NPU to train and infer the InternLM series models.

## News

\[2025.01.07\] InternLM2.5-7B-Chat can be used in transformers.


## Model Zoo

### InternLM2.5

| Model                      | Transformers(HF)                           | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |
| -------------------------- | ------------------------------------------ | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |
| **InternLM2.5-7B-Chat**    | [ðŸ¤—internlm2_5-7b-chat](https://huggingface.co/internlm/internlm2_5-7b-chat) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2_5-7b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-original) | 2024-07-03   |

## Transformers
ä½¿ç”¨æŒ‡å¯¼

ç²¾åº¦

æ€§èƒ½

## LLama-Factory
ä½¿ç”¨æŒ‡å¯¼

ç²¾åº¦

æ€§èƒ½

## xtuner
ä½¿ç”¨æŒ‡å¯¼

ç²¾åº¦

æ€§èƒ½

## LMDeploy
ä½¿ç”¨æŒ‡å¯¼

ç²¾åº¦

æ€§èƒ½

## OpenCompass
ä½¿ç”¨æŒ‡å¯¼

ç²¾åº¦

æ€§èƒ½

## License

The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the [application form (English)](https://wj.qq.com/s2/12727483/5dba/)/[ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰](https://wj.qq.com/s2/12725412/f7c1/). For other questions or collaborations, please contact <internlm@pjlab.org.cn>.